{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxik+aM/wJOS4E63b2JuXn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/WBS2003/blob/main/1_2_Linear_Regression_(ML).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression: Machine learning approach\n",
        "Following on from our [previous Notebook](https://colab.research.google.com/github/MJMortensonWarwick/WBS2003/blob/main/1_1_Linear_Regression_(statistics).ipynb), this tutorial will address the same regression problem, but this time taking a machine learning approach to things.\n",
        "\n",
        "We will begin by importing the packages and data, and setting up our variables (all as before):"
      ],
      "metadata": {
        "id": "SWzterD1smfT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE3NOCNKsiUv",
        "outputId": "597437fe-ca56-438c-e1f2-b7b4a323e5c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X values: \n",
            "         0     1     2   3      4      5     6       7   8      9     10  \\\n",
            "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
            "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
            "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
            "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
            "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
            "\n",
            "       11    12  \n",
            "0  396.90  4.98  \n",
            "1  396.90  9.14  \n",
            "2  392.83  4.03  \n",
            "3  394.63  2.94  \n",
            "4  396.90  5.33  \n",
            "\n",
            "Y value: \n",
            " 0    24.0\n",
            "1    21.6\n",
            "2    34.7\n",
            "3    33.4\n",
            "4    36.2\n",
            "Name: 13, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd  \n",
        "import seaborn as sns \n",
        "\n",
        "# Only works on Jupyter/Anaconda\n",
        "%matplotlib inline  \n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# read in the data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv\", header=None)\n",
        "\n",
        "# serparate the x and Y values\n",
        "x_values = df.drop([13], axis = 1)\n",
        "print(f'X values: \\n {x_values.head()}\\n')\n",
        "\n",
        "y_value = df[13]\n",
        "print(f'Y value: \\n {y_value[0:5]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train-Test Split\n",
        "As per the slides, our next step will be to split the data such that part can be used for training the model, and part can be reserved for testing the model. We can do this with just a couple of lines of code:  "
      ],
      "metadata": {
        "id": "P0t8H8BPth6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into training and test\n",
        "from sklearn.model_selection  import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x_values, y_value, test_size = 0.2, random_state=1234)\n",
        "\n",
        "# print the shapes to check everything is OK\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qylx83LTt31c",
        "outputId": "8df6d4a5-1d60-4f24-8e93-3b85353194e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404, 13)\n",
            "(102, 13)\n",
            "(404,)\n",
            "(102,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our code we have split using an 80:20 ratio (80% of the data for training; 20% for testing). We specify _random\\_state_ as a fixed number so that this split will be the same each time (the splitting algorithm is based on random numbers). We can confirm this has worked by looking at the size of our different datasets:\n",
        "\n",
        "\n",
        "*   _X\\_train_ (the $x$ values we use for training) is 404 rows and 13 columns. 13x columns is what we would expect;\n",
        "*   _X\\_test_ (the $x$ values we use for testing) is 102 rows and 13 columns. Again we expect the 13x columns, but also we can compare the number of rows with _X\\_train_ ... 102 rows is approximately 20% of the total;\n",
        "*   _Y\\_train_ (the $Y$ values we use for training) is 404 rows and a single column - again, what we would expect;\n",
        "*   _Y\\_test_ (the $Y$ values we use for testing) is 102 rows and a single columns. All seems to be correct!\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "###Regularisation\n",
        "Now we can turn our attention to the model. As per the slides, our linear regression will remain the same, but we will use a modified algorithm to determine it. Specifically we will use $L1$ regression (LASSO) where the objective function (cost/loss function) is:\n",
        "\n",
        "$OLS\\ objective + \\alpha \\cdot  \\Sigma |\\beta_i|$\n",
        "\n",
        "I.e. we add a penalty to the ordinary least squares (OLS) objective to limit the sum of the absolute values of the co-efficients ($\\beta$ weights). We can see the amount of influence this penalty has is controlled by the hyperparameter $\\alpha$. Hyperparameters are something we will return to later in the module but for the moment we will just use an arbitrary value of 0.25. \n",
        "\n",
        "With this in mind we can learn our model from the data:\n"
      ],
      "metadata": {
        "id": "ba7QrrlBuDUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1_model = Lasso(alpha=0.25)\n",
        "\n",
        "# fit the model to the training data\n",
        "l1_model_fit = l1_model.fit(X_train, Y_train)\n",
        "\n",
        "# predict the data\n",
        "predict = l1_model_fit.predict(X_test)\n",
        "\n",
        "# calculate R^2 by comparing the predicted values and real values\n",
        "r2 = r2_score(Y_test, predict)\n",
        "print(f'R^2 score is {round(r2, 2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVvBsyo8w6Qu",
        "outputId": "9a765c26-c3f8-4c02-e48a-6ffe03152a58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score is 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that $R^2$ is lower than our previous (statistical) model which achieved 96%. However, we need to recall that in this case we are testing the model against previously unseen data ... a much harder problem. Overall an $R^2$ at 76% is generally going to be considered quite high for this kind of task - so good news. That is not to say that this is the \"optimal\" model, and in practice we would experiment further.\n",
        "\n",
        "You may also notice the conspicuous absence of any hypothesis testing ($p$-values). Typically we would not report or test these in a machine learning approach. We do not have the same underlying (implicit) beliefs of the model as generator of the data, and ultimately no hypotheses to test (in the traditional sense). In essence the model is the hypothesis (our belief that this model will be able to score ($R^2$ in this case) reasonably well on unseen/test data. In machine learning terminology, you will often here the possible models to be tested described as the 'hypothesis space'. Given this change in focus, $p$-values are simply irrelevant.\n",
        "\n",
        "However, we may also want to inspect the $\\beta$ values of the model:"
      ],
      "metadata": {
        "id": "M5STr84UxYeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the beta values of the model (co-efficients)\n",
        "betas_l1 = l1_model_fit.coef_\n",
        "counter = 0\n",
        "for col in x_values.columns:\n",
        "    if counter == 0:\n",
        "        print(\"Beta weights/co-efficients - LASSO\")\n",
        "        print(\"-----------------------------------------\")\n",
        "    print(f\"{col} : {round(betas_l1[counter], 4)}\")\n",
        "    counter +=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei5_fbF516lb",
        "outputId": "5fc9e8df-49b1-475c-d570-f882238c4d52"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beta weights/co-efficients - LASSO\n",
            "-----------------------------------------\n",
            "0 : -0.0831\n",
            "1 : 0.0657\n",
            "2 : -0.0228\n",
            "3 : 0.0\n",
            "4 : -0.0\n",
            "5 : 2.3925\n",
            "6 : -0.0087\n",
            "7 : -1.2545\n",
            "8 : 0.305\n",
            "9 : -0.0165\n",
            "10 : -0.7983\n",
            "11 : 0.011\n",
            "12 : -0.6034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may observe two things. Firstly we can see the $\\beta$ weights for feature 3 and 4 have been set at zero - effectively deleting these features entirely (any number multipled by zero is zero). From the class metaphor, the algorithm has decided these two decision makers are not improving the decision quality and their influence increases the variance of the model. \n",
        "\n",
        "Secondly, we will notice that nearly all of the $\\beta$ weights are smaller (closer to zero) than in the statistical appraoch of the last Notebook. For instance, feature 5 had a $\\beta$ weight of 5.93 in the original model, compared with 2.39 here (less than half). Again, this fits with our metaphor - the algorithm has tried to ensure that no voices are too loud in  our group discussion. I.e. our model will be less likely to over-respond to excess noise in any particular feature as the associate $\\beta$ weight is smaller.\n",
        "\n",
        "So there we have it. Hopefully the comparison of the two approaches gives you some insight into how these two methodologies differ, and how the statistical approach has effectively been adapted to the supervised machine learning problem ... building models that can learn from data. "
      ],
      "metadata": {
        "id": "rZRoDs6l2E88"
      }
    }
  ]
}